{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-requisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fist start with installing nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run ``>>python``in terminal. We import nltk and download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``\n",
    "import nltk\n",
    "nltk.download()\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Phase**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import nltk module to tokenize strings in.\n",
    "\n",
    "We import csv module to read from *SubtaskA_EvaluationData.csv* file and writing the processed data into *Anshuman_Pati.csv*.\n",
    "\n",
    "We import re module for manipulating regular expressions.\n",
    "We import other modules as per requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we assign path variables to the input file and the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'SubtaskA_EvaluationData.csv'\n",
    "out_path = 'Anshuman_Pati.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class ``taggingParsing``. The class contains two functions ``sentenceSplit(self, text)`` and ``taggingNLTK(self, text)``. \n",
    "\n",
    "``sentenceSplit(self,text)`` function splits individual reviews into words for regex matching.\n",
    "\n",
    "``taggingNLTK(self, text)`` function as the name suggests tags the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class taggingParsing:\n",
    "\n",
    "    def sentenceSplit(self, text):\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sentences = tokenizer.tokenize(text)\n",
    "        return sentences\n",
    "\n",
    "    def taggingNLTK(self, text):\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sentences = tokenizer.tokenize(text)\n",
    "        for sent in sentences:\n",
    "            text = word_tokenize(sent)\n",
    "            tagged_sent = nltk.pos_tag(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function ``classify (sent_list)`` where we define several keywords and string patterns (using regular expressions) that may indicate the presence of a suggestion. Next we store the labelled patterns. Default label assigned is 0. If match is made, the label is assigned 1. We return this label_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sent_list):\n",
    "\n",
    "    # Define keywords and regular expressions which indicate a suggestion i.e. X=1. If none of these match, X=0.\n",
    "    \n",
    "    keywords = [\"suggest\",\"recommend\",\"hopefully\",\"go for\",\"request\",\"it would be nice\", \"It would be nice\", \"adding\", \"Adding\", \"Should\",\"could come with\", \"could be\", \"I need\" , \"we need\", \"We need\", \"needs\", \"would like to\",\"would love to\",\"allow\", \"Allow\" ,\"add\", \"Add\", \"don't\", \"please\", \"Please\", \"alternative\", \"should\", \"must\"]\n",
    "\n",
    "    pattern_strings = [r'.*would\\slike.*if.*', r'.*i\\swish.*', r'.*i\\shope.*', r'.*I\\swish.*', r'.*I\\shope.*', r'.*i\\swant.*', r'.*hopefully.*',\n",
    "                       r\".*if\\sonly.*\", r\".*would\\sbe\\sbetter\\sif.*\", r\".*should.*\", r\".*would\\sthat.*\",\n",
    "                       r\".*can't\\sbelieve.*didn't.*\", r\".*don't\\sbelieve.*didn't.*\", r\".*do\\swant.*\", r\".*i\\scan\\shas.*\"]\n",
    "\n",
    "\n",
    "    compiled_patterns = []      \n",
    "    for patt in pattern_strings:\n",
    "        compiled_patterns.append(re.compile(patt))\n",
    "\n",
    "    # Store the labelled patterns\n",
    "    # Default label set to zero\n",
    "    label_list = []\n",
    "    for sent in sent_list:\n",
    "        tokenized_sent = word_tokenize(sent[1])\n",
    "        tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "        tags = [i[1] for i in tagged_sent]\n",
    "        label = 0\n",
    "        patt_matched = False\n",
    "        for compiled_patt in compiled_patterns:\n",
    "            joined_sent = \" \".join(tokenized_sent)\n",
    "            matches = compiled_patt.findall(joined_sent)\n",
    "            if len(matches) > 0:\n",
    "                patt_matched = True\n",
    "        keyword_match = any(elem in keywords for elem in tokenized_sent)\n",
    "        \n",
    "        \n",
    "        pos_match = any(elem in ['MD', 'VB'] for elem in tags)\n",
    "\n",
    "        \n",
    "        # Label set to 1 (indicates 'suggestion') if the given review matches the string or regex expressions\n",
    "        if patt_matched:\n",
    "            label = 1\n",
    "        elif keyword_match == True:\n",
    "                label = 1\n",
    "        elif pos_match == True:\n",
    "                label = 1    \n",
    "     \n",
    "\n",
    "        label_list.append(label)\n",
    "\n",
    "\n",
    "\n",
    "    return label_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the input csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This reads CSV a given CSV and stores the data in a list\n",
    "def read_csv(data_path):\n",
    "    file_reader = csv.reader(open(data_path,\"rt\"), delimiter=',') #, errors=\"ignore\", encoding=\"utf-8\"\n",
    "    sent_list = []\n",
    "\n",
    "    for row in file_reader:\n",
    "        id = row[0]\n",
    "        sent = row[1]\n",
    "        sent_list.append((id,sent))\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the processed csv file to ``out_path``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will create and write into a new CSV\n",
    "def write_csv(sent_list, label_list, out_path):\n",
    "        filewriter = csv.writer(open(out_path, \"w+\"))\n",
    "        count = 0\n",
    "        for ((id, sent), label) in zip(sent_list, label_list):\n",
    "                filewriter.writerow([id, sent, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the ``data_path`` and ``sent_list``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    sent_list = read_csv(data_path)\n",
    "    label_list = classify(sent_list)\n",
    "    write_csv(sent_list, label_list, out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
